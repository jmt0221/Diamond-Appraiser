{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "%matplotlib inline\n",
    "np.random.seed(12)\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=10),\n",
    "            ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True),TQDMNotebookCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv file and drop the unnamed column\n",
    "df = pd.read_csv('diamonds.csv')\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dummy variables and limit price range to $3000\n",
    "df = pd.get_dummies(df,drop_first=True)\n",
    "df = df[df.price < 3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create X and y dataframes\n",
    "X = df.drop('price',axis=1)\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the min and max scalers\n",
    "mm = MinMaxScaler()\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into a train and test and set a split size and random state for reproducability\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the X train and test with Standard scalers\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate Linear Regression then fit and predict\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "ytrain_pred = model.predict(X_train)\n",
    "ytest_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: 34937.9500292992 MSE Test: 36777.269233048966\n",
      "R2 Train: 0.9318507502351661 R2 Test: 0.9281579663289184\n"
     ]
    }
   ],
   "source": [
    "#print out the MSE and R2\n",
    "print('MSE Train:',mean_squared_error(y_train,ytrain_pred), 'MSE Test:',mean_squared_error(y_test,ytest_pred))\n",
    "print('R2 Train:',r2_score(y_train,ytrain_pred),'R2 Test:', r2_score(y_test,ytest_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:55:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "#fit XGBOOST, then predict\n",
    "clfX = XGBRegressor()\n",
    "clfX.fit(X_train,y_train, verbose=True)\n",
    "ytrain_pred = clfX.predict(X_train)\n",
    "ytest_pred = clfX.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: 25824.791992638882 MSE Test: 27456.563724539013\n",
      "R2 Train: 0.9496266896553652 R2 Test: 0.9463653659794307\n"
     ]
    }
   ],
   "source": [
    "#print out the MSE and R2\n",
    "print('MSE Train:',mean_squared_error(y_train,ytrain_pred), 'MSE Test:',mean_squared_error(y_test,ytest_pred))\n",
    "print('R2 Train:',r2_score(y_train,ytrain_pred),'R2 Test:', r2_score(y_test,ytest_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit Gradient Boosted Trees, then predict\n",
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X_train,y_train)\n",
    "ytrain_pred = gbr.predict(X_train)\n",
    "ytest_pred = gbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: 25743.5774961963 MSE Test: 27217.229156715945\n",
      "R2 Train: 0.9497851049887762 R2 Test: 0.946832890688001\n"
     ]
    }
   ],
   "source": [
    "#print out the MSE and R2\n",
    "#print out the MSE and R2\n",
    "print('MSE Train:',mean_squared_error(y_train,ytrain_pred), 'MSE Test:',mean_squared_error(y_test,ytest_pred))\n",
    "print('R2 Train:',r2_score(y_train,ytrain_pred),'R2 Test:', r2_score(y_test,ytest_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest with some parameters to stop overfitting, then predict\n",
    "rf = RandomForestRegressor(n_estimators=100,max_depth=16,max_features=6,min_samples_split=8,min_samples_leaf=6)\n",
    "rf.fit(X_train,y_train)\n",
    "ytrain_pred = rf.predict(X_train)\n",
    "ytest_pred = rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Train: 17227.75190763932 MSE Test: 23697.13345079974\n",
      "R2 Train: 0.9663958999696395 R2 Test: 0.9537091715947577\n"
     ]
    }
   ],
   "source": [
    "#print out the MSE and R2\n",
    "print('MSE Train:',mean_squared_error(y_train,ytrain_pred), 'MSE Test:',mean_squared_error(y_test,ytest_pred))\n",
    "print('R2 Train:',r2_score(y_train,ytrain_pred),'R2 Test:', r2_score(y_test,ytest_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1_maker(X_train,y_train,X_test,y_test,epochs):  \n",
    "    n_cols = X_train.shape[1]\n",
    "\n",
    "    # Set up the model: model\n",
    "    model1 = Sequential()\n",
    "\n",
    "    # Add the first layer\n",
    "    model1.add(Dense(12,  input_shape=(n_cols,)))\n",
    "\n",
    "\n",
    "    # Add the second layer\n",
    "    model1.add(Dense(32,activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=1)))\n",
    "\n",
    "\n",
    "    model1.add(Dense(64,activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=1),kernel_regularizer=regularizers.l2(0.4) ))\n",
    "    model1.add(Dense(64,activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=1),kernel_regularizer=regularizers.l2(0.3) ))\n",
    "\n",
    "\n",
    "\n",
    "    model1.add(Dense(64,activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=1),kernel_regularizer=regularizers.l2(0.5) ))\n",
    "    model1.add(Dense(64,activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=1),kernel_regularizer=regularizers.l2(0.4) ))\n",
    "\n",
    "    model1.add(Dense(32,activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=1)))\n",
    "\n",
    "    model1.add(Dense(16,activation='relu', kernel_initializer=keras.initializers.glorot_uniform(seed=1)))\n",
    "\n",
    "    # Add the output layer\n",
    "    model1.add(Dense(1))\n",
    "\n",
    "    model1.compile(optimizer='adam',loss='mean_squared_error')\n",
    "    history1 = model1.fit(X_train,y_train,epochs=epochs,batch_size=64,validation_data=(X_test,y_test),verbose=2,callbacks=callbacks)\n",
    "    \n",
    "    \n",
    "    training_loss = history1.history['loss']\n",
    "    test_loss = history1.history['val_loss']\n",
    "\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "    # Visualize accuracy history\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    plt.plot(epoch_count, training_loss, 'r--')\n",
    "    plt.plot(epoch_count, test_loss, 'b-')\n",
    "    plt.legend(['Training Loss', 'Test Loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.ylim(top=40000)\n",
    "    plt.show();\n",
    "    \n",
    "    return model1, history1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2_maker(X_train,y_train,X_test,y_test,epochs):  \n",
    "    n_cols = X_train.shape[1]\n",
    "\n",
    "    # Set up the model: model\n",
    "    model2 = Sequential()\n",
    "\n",
    "    # Add the first layer\n",
    "    model2.add(Dense(32, activation='relu',kernel_initializer = 'uniform', input_shape=(n_cols,)))\n",
    "\n",
    "    # Add the second layer\n",
    "    model2.add(Dense(64,kernel_initializer = 'uniform',activation='relu'))\n",
    "\n",
    "    model2.add(Dense(128,kernel_initializer = 'uniform',activation='relu',kernel_regularizer=regularizers.l2(0.1)))\n",
    "\n",
    "    model2.add(Dense(64,activation='relu',kernel_initializer = 'uniform'))\n",
    "\n",
    "\n",
    "    model2.add(Dense(32,activation='relu',kernel_initializer = 'uniform'))\n",
    "\n",
    "    # Add the output layer\n",
    "    model2.add(Dense(1))\n",
    "\n",
    "    model2.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n",
    "    history2 = model2.fit(X_train,y_train,epochs=epochs,batch_size=64,validation_data=(X_test, y_test),verbose=2,callbacks=callbacks)\n",
    "    \n",
    "    \n",
    "    training_loss = history2.history['loss']\n",
    "    test_loss = history2.history['val_loss']\n",
    "\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "    # Visualize accuracy history\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    plt.plot(epoch_count, training_loss, 'r--')\n",
    "    plt.plot(epoch_count, test_loss, 'b-')\n",
    "    plt.legend(['Training Loss', 'Test Loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.ylim(top=40000)\n",
    "    plt.show();\n",
    "    \n",
    "    \n",
    "    return model2, history2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3_maker(X_train,y_train,X_test,y_test,epochs):  \n",
    "    n_cols = X_train.shape[1]\n",
    "\n",
    "    # Set up the model: model\n",
    "    model3 = Sequential()\n",
    "\n",
    "    # Add the first layer\n",
    "    model3.add(Dense(128, activation='relu',kernel_initializer = keras.initializers.glorot_uniform(seed=1),kernel_regularizer=regularizers.l2(0.3), input_shape=(n_cols,)))\n",
    "\n",
    "    model3.add(Dense(256,kernel_initializer = keras.initializers.glorot_uniform(seed=1),activation='relu',kernel_regularizer=regularizers.l2(0.3)))\n",
    "\n",
    "    model3.add(Dense(256,kernel_initializer = keras.initializers.glorot_uniform(seed=1),activation='relu',kernel_regularizer=regularizers.l2(0.3)))\n",
    "\n",
    "    model3.add(Dense(128,activation='relu',kernel_initializer = keras.initializers.glorot_uniform(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "\n",
    "    model3.add(Dense(64,activation='relu',kernel_initializer = keras.initializers.glorot_uniform(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "\n",
    "\n",
    "    # Add the output layer\n",
    "    model3.add(Dense(1))\n",
    "    model3.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n",
    "    history3 = model3.fit(X_train,y_train,epochs=epochs,batch_size=64,validation_data=(X_test, y_test),verbose=2,callbacks=callbacks)\n",
    "    \n",
    "    \n",
    "    training_loss = history3.history['loss']\n",
    "    test_loss = history3.history['val_loss']\n",
    "\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "    # Visualize accuracy history\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    plt.plot(epoch_count, training_loss, 'r--')\n",
    "    plt.plot(epoch_count, test_loss, 'b-')\n",
    "    plt.legend(['Training Loss', 'Test Loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.ylim(top=40000)\n",
    "    plt.show();\n",
    "    \n",
    "    return model3, history3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model4_maker(X_train,y_train,X_test,y_test,epochs):      \n",
    "    n_cols = X_train.shape[1]\n",
    "\n",
    "    # Set up the model: model\n",
    "    model4 = Sequential()\n",
    "\n",
    "    # Add the first layer\n",
    "    model4.add(Dense(32,activation='relu',kernel_regularizer=regularizers.l2(0.3) ,kernel_initializer = keras.initializers.glorot_normal(seed=1), input_shape=(n_cols,)))\n",
    "\n",
    "\n",
    "    model4.add(Dense(32,activation='relu', kernel_initializer = keras.initializers.glorot_normal(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "\n",
    "\n",
    "    model4.add(Dense(64,activation='relu', kernel_initializer = keras.initializers.glorot_normal(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "\n",
    "\n",
    "    model4.add(Dense(128,activation='relu', kernel_initializer = keras.initializers.glorot_normal(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "\n",
    "\n",
    "    model4.add(Dense(64,activation='relu', kernel_initializer = keras.initializers.glorot_normal(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "\n",
    "    model4.add(Dense(32,activation='relu', kernel_initializer = keras.initializers.glorot_normal(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "\n",
    "    model4.add(Dense(16,activation='relu', kernel_initializer = keras.initializers.glorot_normal(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "\n",
    "\n",
    "    # Add the output layer\n",
    "    model4.add(Dense(1))\n",
    "    model4.compile(optimizer='adam',loss='mean_squared_error')\n",
    "    history4 = model4.fit(X_train,y_train,epochs=epochs,batch_size=64,validation_data=(X_test, y_test),verbose=2,callbacks=callbacks)\n",
    "\n",
    "    \n",
    "    training_loss = history4.history['loss']\n",
    "    test_loss = history4.history['val_loss']\n",
    "\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "    # Visualize accuracy history\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    plt.plot(epoch_count, training_loss, 'r--')\n",
    "    plt.plot(epoch_count, test_loss, 'b-')\n",
    "    plt.legend(['Training Loss', 'Test Loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.ylim(top=40000)\n",
    "    plt.show();\n",
    "    \n",
    "    return model4, history4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model5_maker(X_train,y_train,X_test,y_test,epochs):  \n",
    "    n_cols = X_train.shape[1]\n",
    "\n",
    "    # Set up the model: model\n",
    "    model5 = Sequential()\n",
    "\n",
    "    # Add the first layer\n",
    "    model5.add(Dense(32,activation='linear',kernel_regularizer=regularizers.l2(0.3) ,kernel_initializer = keras.initializers.glorot_normal(seed=1), input_shape=(n_cols,)))\n",
    "    model5.add(keras.layers.LeakyReLU(alpha=0.05))\n",
    "\n",
    "    model5.add(Dense(32,activation='linear', kernel_initializer = keras.initializers.glorot_normal(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "    model5.add(keras.layers.LeakyReLU(alpha=0.05))\n",
    "\n",
    "\n",
    "    model5.add(Dense(64,activation='linear', kernel_initializer = keras.initializers.glorot_normal(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "    model5.add(keras.layers.LeakyReLU(alpha=0.05))\n",
    "\n",
    "\n",
    "    model5.add(Dense(128,activation='linear', kernel_initializer = keras.initializers.glorot_normal(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "    model5.add(keras.layers.LeakyReLU(alpha=0.05))\n",
    "\n",
    "\n",
    "    model5.add(Dense(64,activation='linear', kernel_initializer = keras.initializers.glorot_normal(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "    model5.add(keras.layers.LeakyReLU(alpha=0.05))\n",
    "\n",
    "    model5.add(Dense(32,activation='linear', kernel_initializer = keras.initializers.glorot_normal(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "    model5.add(keras.layers.LeakyReLU(alpha=0.05))\n",
    "\n",
    "    model5.add(Dense(16,activation='linear', kernel_initializer = keras.initializers.glorot_normal(seed=1),kernel_regularizer=regularizers.l2(0.3)))\n",
    "    model5.add(keras.layers.LeakyReLU(alpha=0.05))\n",
    "\n",
    "\n",
    "    # Add the output layer\n",
    "    model5.add(Dense(1))\n",
    "    model5.compile(optimizer='adam',loss='mean_squared_error')\n",
    "    history5 = model5.fit(X_train,y_train,epochs=epochs,batch_size=64,validation_data=(X_test, y_test),verbose=2,callbacks=callbacks)\n",
    "\n",
    "    training_loss = history5.history['loss']\n",
    "    test_loss = history5.history['val_loss']\n",
    "\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "    # Visualize accuracy history\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    plt.plot(epoch_count, training_loss, 'r--')\n",
    "    plt.plot(epoch_count, test_loss, 'b-')\n",
    "    plt.legend(['Training Loss', 'Test Loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.ylim(top=40000)\n",
    "    plt.show();\n",
    "    return model5, history5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model6_maker(X_train,y_train,X_test,y_test,epochs):     \n",
    "    n_cols = X_train.shape[1]\n",
    "\n",
    "    # Set up the model: model\n",
    "    model6 = Sequential()\n",
    "\n",
    "    # Add6the first layer\n",
    "    model6.add(Dense(32, activation='relu',kernel_initializer = keras.initializers.glorot_normal(seed=None), input_shape=(n_cols,)))\n",
    "\n",
    "    model6.add(Dense(32,kernel_initializer = keras.initializers.glorot_normal(seed=None),activation='relu'))\n",
    "\n",
    "    model6.add(Dense(64,kernel_initializer = keras.initializers.glorot_normal(seed=None),activation='relu'))\n",
    "\n",
    "    model6.add(Dense(128,activation='relu',kernel_initializer = keras.initializers.glorot_normal(seed=None),kernel_regularizer=regularizers.l2(0.1)))\n",
    "\n",
    "    model6.add(Dense(64,kernel_initializer = keras.initializers.glorot_normal(seed=None),activation='relu'))\n",
    "\n",
    "    model6.add(Dense(32,activation='relu',kernel_initializer = keras.initializers.glorot_normal(seed=None)))\n",
    "\n",
    "    model6.add(Dense(16,kernel_initializer = keras.initializers.glorot_normal(seed=None),activation='relu'))\n",
    "\n",
    "    # Add6the output layer\n",
    "    model6.add(Dense(1))\n",
    "\n",
    "    model6.compile(optimizer='adam',loss='mean_squared_error')\n",
    "    history6 = model6.fit(X_train,y_train,epochs=epochs,batch_size=64,validation_data=(X_test, y_test),verbose=2,callbacks=callbacks)\n",
    "    \n",
    "    training_loss = history6.history['loss']\n",
    "    test_loss = history6.history['val_loss']\n",
    "\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "    # Visualize accuracy history\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    plt.plot(epoch_count, training_loss, 'r--')\n",
    "    plt.plot(epoch_count, test_loss, 'b-')\n",
    "    plt.legend(['Training Loss', 'Test Loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.ylim(top=40000)\n",
    "    plt.show();\n",
    "    \n",
    "    \n",
    "    return model6, history6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_layer(preds_train,preds_test,y_train,y_test,epochs):\n",
    "    n_cols = preds_train.shape[1]\n",
    "\n",
    "    # Set up the model: model\n",
    "    model_ensemble = Sequential()\n",
    "\n",
    "    # Add the first layer\n",
    "    model_ensemble.add(Dense(6, activation='relu',kernel_initializer = keras.initializers.glorot_normal(seed=1), input_shape=(n_cols,)))\n",
    "\n",
    "    model_ensemble.add(Dense(6,kernel_initializer = keras.initializers.glorot_normal(seed=1),activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "    # Add the output layer\n",
    "    model_ensemble.add(Dense(1))\n",
    "\n",
    "    model_ensemble.compile(optimizer='adam',loss='mean_squared_error')\n",
    "    history_ensemble = model_ensemble.fit(preds_train,y_train,epochs=epochs,batch_size=64,\n",
    "                                          validation_data=(preds_test, y_test),verbose=2,callbacks=callbacks)\n",
    "    \n",
    "    training_loss = history_ensemble.history['loss']\n",
    "    test_loss = history_ensemble.history['val_loss']\n",
    "\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_accuracy) + 1)\n",
    "\n",
    "    # Visualize accuracy history\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    plt.plot(epoch_count, training_loss, 'r--')\n",
    "    plt.plot(epoch_count, test_loss, 'b-')\n",
    "    plt.legend(['Training Loss', 'Test Loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.ylim(top=40000)\n",
    "    plt.show();\n",
    "    \n",
    "    \n",
    "    return model_ensemble,history_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_ensemble(X_train,y_train,X_test,y_test,epochs):\n",
    "    model1,history1 = model1_maker(X_train,y_train,X_test,y_test,epochs)\n",
    "    model2,history2 = model2_maker(X_train,y_train,X_test,y_test,epochs)\n",
    "    model3,history3 = model3_maker(X_train,y_train,X_test,y_test,epochs)\n",
    "    model4,history4 = model4_maker(X_train,y_train,X_test,y_test,epochs)\n",
    "    model5,history5 = model5_maker(X_train,y_train,X_test,y_test,epochs)\n",
    "    model6,history6 = model6_maker(X_train,y_train,X_test,y_test,epochs)\n",
    "    \n",
    "    predictions1_train = pd.DataFrame(model1.predict(X_train))\n",
    "    predictions2_train = pd.DataFrame(model2.predict(X_train))\n",
    "    predictions3_train = pd.DataFrame(model3.predict(X_train))\n",
    "    predictions4_train = pd.DataFrame(model4.predict(X_train))\n",
    "    predictions5_train = pd.DataFrame(model5.predict(X_train))\n",
    "    predictions6_train = pd.DataFrame(model6.predict(X_train))\n",
    "    \n",
    "    predictions1_test= pd.DataFrame(model1.predict(X_test))\n",
    "    predictions2_test= pd.DataFrame(model2.predict(X_test))\n",
    "    predictions3_test= pd.DataFrame(model3.predict(X_test))\n",
    "    predictions4_test= pd.DataFrame(model4.predict(X_test))\n",
    "    predictions5_test= pd.DataFrame(model5.predict(X_test))\n",
    "    predictions6_test= pd.DataFrame(model6.predict(X_test))\n",
    "    \n",
    "    \n",
    "    train_preds = pd.concat([predictions1_train,predictions2_train,predictions3_train,predictions4_train,\n",
    "                        predictions5_train,predictions6_train],axis=1)\n",
    "    test_preds = pd.concat([predictions1_test,predictions2_test,predictions3_test,predictions4_test,\n",
    "                        predictions5_test,predictions6_test],axis=1)\n",
    "    train_preds = ss.fit_transform(train_preds)\n",
    "    test_preds = ss.transform(test_preds)\n",
    "    \n",
    "    model_ensemble, history_ensemble = ensemble_layer(train_preds,test_preds,y_train,y_test,epochs)\n",
    "    \n",
    "    return model_ensemble, history_ensemble, train_preds, test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all the models indivdually then together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1,history1 = model1_maker(X_train,y_train,X_test,y_test,epochs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2,history2 = model2_maker(X_train,y_train,X_test,y_test,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3,history3 = model3_maker(X_train,y_train,X_test,y_test,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4,history4 = model4_maker(X_train,y_train,X_test,y_test,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5,history5 = model6_maker(X_train,y_train,X_test,y_test,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6,history6 = model6_maker(X_train,y_train,X_test,y_test,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ensemble, history_ensemble, train_preds, test_preds = stack_ensemble(X_train,y_train,X_test,y_test,epochs=100)\n",
    "\n",
    "#ensembled models MSE and R2\n",
    "print(mean_squared_error(ytrain_preds,y_train), mean_squared_error(ytest_preds,y_test))\n",
    "print(r2_score(ytrain_preds,y_train), r2_score(ytest_preds,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ensemble.save('ensembled_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
